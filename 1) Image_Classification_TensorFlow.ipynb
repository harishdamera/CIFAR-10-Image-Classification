{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "Classifing images from CIFAR-10 dataset using CNN. It consists of airplanes, dogs, cats, and other objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "The dataset is broken into 5 batches to prevent machine from running out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 19:\n",
      "Image - Min Value: 9 Max Value: 242\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 6 Name: frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAG75JREFUeJzt3UmPbed1HuB12uqb29Vt2IgURUqMpDiJ49j5LUECJAP/\nr/yDjAI4QAAZsaEgUSArlihRYiPxkrxd3aZu9XW6TDIQAniwVkqisfA884W1zz7f2e/Zo3ewWq0C\nAOhp+E1fAADwhyPoAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoA\naEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQ2/qYv4A/l3pt3V6W5g1vpmcnitLIqPnjzZnrmn374RmnX\nv/rTH5bmfv7Jo/TMf/xP/6W06+DgTnrm3Xu3S7s2ptPS3GKxTM8cHByUdk3G+Z/n8vyqtOu7H+bP\nx4ureWnXJ199WZobTSfpmW89uF/adfdG4Sy++2Fp12cPn5Xm/ut/+3F6ZrkqPRbj/ffeTc+8fP68\ntOtnP/tZae72rfyz+6039kq7/vRf/Ov0zLvv/EVp1/e+f39QGvw93ugBoDFBDwCNCXoAaEzQA0Bj\ngh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaa9tetzGu/YcZxyI/M6rdxtOL\nfBPa8Vl+JiLi/CL/uSIihoX78dad/dKu+7fzczeKR3h5Xmte+/pVvmlssjkq7br3rXxT4eu12vf8\n68sX6ZkXr85Lu86jdoYf7G+nZ/b28jMREWuT/Llam9bO4qj4ujUa5kvN5lez0q7FIn+u5oWZiIhV\nsWGvMnXzZu1ZdfDgbnrmavLNvVd7oweAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0Jig\nB4DGBD0ANCboAaAxQQ8AjfUttZnmCx8iItbXN9Izg2HtNj67yP/P+slntSKRz5/+r9LcanGRnjk6\nrpWW7K7li2ZGk/z1RURcXV2W5va299Iz26Npadfh5w/TM6viWZwMb6Znhs9q9350Vpvb3sqfq53R\npLRra2szPXNxXvtt7m5vlea2NtbTMxeXtXtfUiynGQxqz+7K3GnxLF5c5Qt7Vlu1z3UdvNEDQGOC\nHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA01ra9bjGv\nNaitb95Oz9y4+15p19UsPzNcqzVd/fzTn5XmXjz+Ij0zOzsq7Xry9ev0zO2d2v3Yv5Fv/oqIePvG\ndnpmWDuKMbzKD87Htcawy+N8s9bFReEAR8TVtPZ+8ewq3w63+fxJadfO3p30zPnZWWnXIGrf2TDy\nDWrDVa1BrTK3WtZ21e5GlF5bL85rLZaz83zr3a37a6Vd18EbPQA0JugBoDFBDwCNCXoAaEzQA0Bj\ngh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBorG2pzWhUKxDY3t7NDw1HpV2XF/kSl/PXL0u7\nlqtaAclobZKemc9qx+pkma+zWF+v3fvd/Xw5TUTEfLyZnjkZ3ajt2n8zPbOxVSv5ie1CicuyVuIS\n81ptyWw2Tc9cXdXeZY5Or9Iza9PauT8vlPVERJxf5OeGg9r9GFS+smo7TbF4JyI/N12v5cRwmP9w\nrw4flXbF23u1ud/jjR4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0Jig\nB4DGBD0ANCboAaCxtu1183lt7vmzJ+mZ8yfPSruGhUa5tWKx086ydkPuPXiQnjk5qbW1nZ9fpmfu\n3au1T91/d6c0d2Mv3/L2bLJR2nU530/PTPO3MCIi1qf5M7zcyTe8RURMzostb1v5fcfL2rvM4VH+\nRq6vr5d2nV3WmiVPL/O/6fEw30YZUSuiW5Xr64oPuUH+XL0+r92Pw+P8Nd7fqt6P/3/e6AGgMUEP\nAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABpr2163s7tZ\nmlssLtIzJ8dnpV2ry5P0zM2drdKuQfGrPj/Kz1xeFKsDC9c43a21163N899zRMTd1/nv7Oy7tbP4\nt09/l54Zzmr3/s+3d9Mzbz7K34uIiNHz0lis7uUb5Y4He6Vdm1f5d6CrYe03dlY7ijFf5a9xMqy+\n2xUa5Va1trZ6x9soPXGy3C5t+vTJeXpmc69YLXkNvNEDQGOCHgAaE/QA0JigB4DGBD0ANCboAaAx\nQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMbaltp88P0flOaODg/TM08fPy7tevPB7fTM9natIOXh\n00I7TUScnpzmZ17nZyIitnfyBTUXVxulXU9O8qUUERHnk6fpmS9P82UbERFfjfLXOLpVK/l5NrxK\nz+z+9qvSrt0XxXt/dz89s7p5q7RrtDNJz2ys1wpSLi5qrTbLynvasFBOExGVqpnlalnbVCzDqRTv\nHOePfUREfPH0dXrm1l5+5rp4oweAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DG\nBD0ANCboAaAxQQ8AjQl6AGisbXvd63mtOen1cb557eBWvlUrIuKNN95Iz4wm+VatiIjD03lpbhmX\n6Zk377xd27WW/84eHS9Ku44PbpbmPtnLt8O9upiWdt3cfZAfWqu1G352fpKeOfnem6VdB5NZae54\nnm8BPDitNcONjj9Kz9y/Uftcy2XtfWt2lf+9DNZqu1aF9rrVsvbbrFoWWu9u79eeA2+8/Z30zP7u\nXmnXdfBGDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAa\na1tqczG/Ks2t7e6mZ5aX+aKTiIjPvjpMz1zMasUZk1G+8CEi4mArf0RuFopfIiKO1vKlJYezF6Vd\njz7/pDS3t7Odn9l6Vdo1uneUntl857ulXdMb+SKik/37pV1/Hy9Lc/cu8yUu+5Nayc9vf/5peubo\nNF8AFRFxt1JeFBGjQhnOeFB95OefH8tl7ZmzKpTTREQMCjPnL5+Wdj28yOfLvb1/Xtp1HbzRA0Bj\ngh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANNa2ve7J\no69Kc2uj/C3Z29oq7srPDMeFoYgYj2v/6dYKlVBnFyelXS9OXqdn3n2wKO166718C11ExJ2376Zn\nbu3dKe06fZ5vN/z6+d+Udr1a3EvP7GzkZyIi7q7X2sn25hfpmV9/dVrade+9b6Vnbk1vlHYtjmpN\nm6PCo2BQu/WlJrrFYl5bVlV4Vr06rOXEs+cfpWc++OCt0q7r4I0eABoT9ADQmKAHgMYEPQA0JugB\noDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgsbbtdc+fPC7NjYb5CqT5jd3SrvsH\nB+mZzbVJadfZRa3l7fXFLD0zWNVaq6Y7+Wtce6vWQnf0xkZtbj9/Ph6tnZd2vf+Dt9MzfzapncWT\nV8/zQ2dflnZtT26W5v7z/3yYnvnlk8vSrnf+7C/SM99598PSrud//+vS3JMv8r/NZfHdbrlcpmfm\n8/xMREShKC8iSuV1MRrUnlXz2Vl6plg8ei280QNAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCN\nCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxtqW2ty6ca80tyqUN6xv7JR2zVbr6Zmt9c3Srps7ta96Msjf\nj7OTSr1ExPmNR+mZrbdru8Y310pzx6vT/MzyqrTrs6v8vX82zBedRETcuXcjPXOwVSunOTk+LM3d\nusj/Xj44qJUXvVrkv+cvi8VRe8Vr3L6fvx9xVny3KxTN1CptIhbVS1zlN65NayVh334nXzj17Osv\nSruugzd6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCY\noAeAxtq21/27f/OXf7Rdo9EfrxFqMBzVdhXHpuP8RZ5f1Hb96viv0zO7D56Vdi22ak1j00m+qXCy\nVmvI2l6bpmemkW9di4hYrC7TM1fj7dKu54vnpbkH383f+6PRWWnXV7/4SXpm9vWL0q6bN98sza3f\n2krP7BTP/Wo5L8zUWhsHxd6745OT9My4+DxdW883B376m1+Udl0Hb/QA0JigB4DGBD0ANCboAaAx\nQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoLG2pTavj57WBgtFM8Niqc1gMCjM1HbN\nKx8sIkbjfAnGq9Naq83ZWr4UZH88K+2a17o94ubNg/TMxka+jCUiYm2V/65vbOyWdm2un6dn5vFl\naddoWCveOTnaT88sF7VH3PlZvixptZc/GxERk+3aNR6d5stfDs8OS7veWss/q+arfBFORMRqWSu1\nmc3z+6ZrtXs/KFxjtaznOnijB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYE\nPQA0JugBoDFBDwCNCXoAaKxte91yUWs1Wy3zLW+rQsvY/x1MjwwGtRa6ZeTbpyIiBov83NGrV6Vd\nZ9v5VrP14gkeDmv1dYvTfMtbDGrtddPxND90Uftcy8IZHo+2S7vevrVZmjsa3UrPnJ2dlHY9uZef\nOSuWkx1s155Vpy9G6ZlHi7XSrsXJy/TMcHFZ2rUsNm0OC8+48bDYXle4xMWsWJl5DbzRA0Bjgh4A\nGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DG2pbavHr1vDS3KhXN\n1P4vDSo9M8Na4cN0WWzcGOVLMB6/eFJadXZ5kZ7ZXN4p7XpwM78rIuKkcD5enj4t7ZoVDsh4525p\n13iUP8Nrw73Srv2tQmNMRGxOt9Izvzt5WNp1cpUvZvr668PSrjcPHpTmNtY30jOzwm8sIuLJfJ6e\n2dvMl+5EREw3asU7q0pnTOkhHLG+XrnG2rP7OnijB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4Ie\nABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaKxte93Wdr7pKiIiSu11tQakKLTeDYa1/2brUal2\nirgaTNIzx1/nm64iIh4/PUnPfPm7WkPW/m7t6G+v52e27two7RoWzscwamdxVSjjWkxr9/58VWtS\nnC/z+7ZHm6Vda7P8vT98cVra9cnT16W59x5sp2dmw+K937yVnrl9t9YsubvxrDT36rPP80PDwg86\nImIwTY9UmlGvizd6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4A\nGhP0ANCYoAeAxtq2103G+XahiFJ5Xdmg0l4XtcawwbDWKHe+zLddHc1rDVmXw3wb16cPz0q77t+v\ntRvu38gfkNXqqrRrNM03ry3HtTaus+FlemY4qLW1HZ0el+Y2Yj89863tndKu9cv87+Wy2Bz41eva\nfRyND9Mzk2Htfrwa3k3PrD78l6Vdm2/8tjT3+tGX6ZlB8Vn1TTbRVXijB4DGBD0ANCboAaAxQQ8A\njQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNtS21KXSxRETEavnHKysYDPK7hoWZ\niIjFsHZDTuaF0pittdKujWm+OOOLR1+Vdn3xRe0/7uw8X/6ycStfThMRsdzJX+NiUisvOr04Ss/M\nly9Kuw6290pz01m+NGZ4uijtGlzlv+fN/ZulXSeLWhnOw6+fpmeGtT6hGG7kP9vRrz4u7bq9kf9c\nERH7Nwq/s+NaBA4G+e+sMnNdvNEDQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANA\nY4IeABoT9ADQmKAHgMYEPQA01ra9brWqtbWtVn+89rooNNEti7V8oxiV5k7Oz9Mz58uL0q7p9m56\n5uRRrRnuk49npbnB/Co9s3VZa3nb38/PrG1NSrsm4/x3Nq4Vw8XyJN8MFxFxPs+/l/zuN1+Wdj19\nlK95G2zWWvk25rWzeHOavx+/ffJFadf2ev47m4xq5/7p2ePS3Pi00DgYhR9ZRFzO8vd+Mqn9Nq+D\nN3oAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0FjbUpvp\neKc0Vyq1GZRWxWCQHxwM5qVdo0ntq75a5K9xvLFe2rUa50sfFsX/qp/95lVpbrLIlwO9/+G90q75\nIF9gNDk+K+1aG+d3jQe1ko7lrFZq8+r0JD1z+NtascpgmS9Y2tmuldrcv7VRmjtY5p9VD88/Le06\nWRymZz5+XDuLGxe1Z9y70/x3tr22Vtq1qETnslYsdh280QNAY4IeABoT9ADQmKAHgMYEPQA0JugB\noDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADTWtr3uajkrzS2X+RavYnldDIb5yVFx2dVg\nUZo7nefndm4elHZN9vKNg6tXtRa6veFWae6Ljx6lZz7/1f8u7frgu/n7+K2DfINXRMSd3fz92N6s\nNX8tL2vvF7OzfFvbtNBCFxExnebP4vp2bddGse3xxacP0zOXL/INgBERy/VpeubitNZSuLlzuzR3\n78330jM317dLu0olp4W20uvijR4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAa\nE/QA0JigB4DGBD0ANCboAaCxtu11l1cXpblSe12xlagyNxzW/ptdzeelueOr/Nxk805p12yRvx/b\nkxulXX/+g2+X5o5vH6Zn/uqv/6q062//5vP0zEd7tSa0/Z3N9MzOZm3XuND8FRExWOR/m8eLWmvj\n/K276ZnbG7WzOL04Ls19+cVX6ZnjZ7W2x1GhqXDnbv4eRkR88J0PSnNv3n87PTM6rz0XB4X6utHo\nm4tbb/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoLG2\npTaD0bQ0N458WcFkOCrtWi7zhRuDYe0rOyqW/BydX6ZnLp7mi18iImarfGnJwaJ2749en5Xmnr3I\nf7bx2kZp12Se/2yLSW3Xw5en+aFnr0u7BoXiqIiIWOV/L7ONSWnVOx/spGc2p7WSn+Vl7T4ez67S\nM6M7e6VdMcoXTq3t1s7izu38vY+IWA5n6Zndjdp3tlH4TY9GtWfVdfBGDwCNCXoAaEzQA0Bjgh4A\nGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0Fjb9rqT05PS3GwxT8+cR75V\nKyLiZJ5vn5ot8y1SERFfvao1ZD27Ok/PrIpNedNCQ9aTeb5dLyLi4ccfleZWr4/TM4NV/kxFRKxG\n+XO1vKo1w20UmrUuVrXHx2JSO8PjjXwj5Y1790u75pO19Myjw1pr48FOfldExP4b76RnFuvPS7vG\nw/w74e1bt0u7Pv7ks9LcnW9/Pz+zs1/aNZzkz+Kg2HJ6HbzRA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm\n6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DG2pbafP97PyzNPZ+dpmd++fTL0q6nr56lZ67m\nq9Kul8ta0cxonC8g2VnfKO1aXOQLatb3bpR23XvwdmnuTqHIZVUoSoqIOB7kS49WxQKdw5f50qNX\nxfeEi7VaicvN+3fTM//kg/dLux4d5kuxfvtl7TmwO9gtzb1x8430zOVhvqQqImJrtJ6e2R5sl3Yd\nHuWfixERq7X8vrtvvVXaNR3kz/54PCntug7e6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DG\nBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABobrFa1NrR/7P7HT39V+mCXs3xj2PFlrRnu9VW+Sep8\nNivtuprnP1dExNHxq/TMxbzWoLa2sZWe2d7eK+3aHtSapAan+e/64rx2Plbr+Wt8PTsr7frs8eP0\nzFHxPeHx8XFpbm8//12/ebt2Pn7y079Lz4wntTP1J2/nW+giIn74zgfpmcki30YZEbExyM9NN2st\nlstR7Rpvb+VbAO/v1JoD16f573p9o3Y/7t3Zqd2Q3+ONHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8A\njQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoLG27XU//btflD7YeDhKz6xtrFdWxWCU/5+1\nWtWKjCbFRqjlMt+WN5svSrtmhc82jNrnGgxrc5VPNhjU/k8PC//DF8vavT8rtCJeFd8TZpe1JsW4\nys8tR8vSqqNCA+NgVdv1TqGVLyLiYOtGemZjbVraNRnmH6ezYtNmrNVaAMeF278q/l6evXyenvnv\nP/5xade//Q//XnsdAPAPE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAH\ngMYEPQA0Nv6mL+AP5eNf/bw0t7m5kZ6ZTGolDINh/n/WZFwrpRgXr3FUuMbxuFhKMc4fx9EoX0IU\nERHFopnJNP/ZqvdjUKjQGRZLqrYH+bnppHbvR5s7pblKD9FiUSstmS/ypTZHL1+Udk1nl6W58SA/\nN4ra+fjlR79Mz/zoRz8q7Xr32++W5t7/zvvpmctZ/nuOiHjy7Fl65mXxfFwHb/QA0JigB4DGBD0A\nNCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNtW2vW62WpbmTk5Nr\nvpJ/2GCQr+MaDmuNYZVd1blhsVFuXJgrltDVFdrhKi2FEbV7f3VVa+MaFP7zb21vl3ZtbW2V5ir3\nY7UsttddXaVnXjzPN5pFRGxtrJfmzk+P0zOLZe25+PjZYXpm//ad0q5BsaHzyfOX+aFR7be5VjjD\nP/iTf1badR280QNAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCY\noAeAxtqW2mxsbJTmlsXSh4pKzUy1nKZa8hOVUpvaJUZE/hpXi+rnKo5V7n+hCKc6NimWdFTKkhaz\ny9Ku46N8YUxE7bdZPoqrfBnO+tqktGpQOPcRESevj9Izy9pRjL393fTMrVs3S7uqz6pl4QezitoN\nqewaDf942fL/8kYPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANA\nY4IeABoT9ADQ2GBVbNYCAP7x80YPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8A\njQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeA\nxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANA\nY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxv4PQPJ/x7d0\nRHwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a9ac557ba8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "batch_id = 1\n",
    "sample_id = 19\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing input images into a range of 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x_min=np.min(x)\n",
    "    x_max=np.max(x)\n",
    "    return (x-x_min)/(x_max-x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(x):\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    enc = OneHotEncoder(n_values=10)    \n",
    "    one_hot_encoded_labels = enc.fit_transform(np.array(x).reshape(-1, 1)).toarray()\n",
    "    return one_hot_encoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import helper\n",
    "\n",
    "# Loading Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    # Tensor for image input\n",
    "    # image_shape = input image details\n",
    "    return tf.placeholder(tf.float32,shape=((None,)+image_shape),name='x') \n",
    "    \n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    # Tensor for label input\n",
    "    # n_classes = number of target classes\n",
    "    return tf.placeholder(tf.float32,shape=(None,+n_classes),name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    # Tensor for Dropout\n",
    "    # keep_prob = dropout probability\n",
    "    return tf.placeholder(tf.float32,shape=(None),name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "\n",
    "    weights_shape=list(conv_ksize)+[x_tensor.get_shape().as_list()[3],conv_num_outputs]\n",
    "    weights=tf.Variable(tf.truncated_normal(weights_shape,stddev=5e-2))\n",
    "    bias=tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    #Convolution layer\n",
    "    output=tf.nn.conv2d(x_tensor,weights,strides=[1,conv_strides[0],conv_strides[1],1],padding='SAME')\n",
    "    output=tf.nn.bias_add(output,bias)\n",
    "    output=tf.nn.relu(output)\n",
    "    \n",
    "    #Pooling layer\n",
    "    # Implemented max pooling with padding around the input image and stride is 1\n",
    "    output=tf.nn.max_pool(output,ksize=[1,pool_ksize[0],pool_ksize[1],1],\n",
    "                          strides=[1,pool_strides[0],pool_strides[1],1],\n",
    "                          padding='SAME')\n",
    "    return output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening Layer\n",
    "Function to change the dimension of x_tensor from a 4-D tensor to a 2-D tensor (Batch Size, Flattened Image Size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    tensor_shape=x_tensor.get_shape().as_list()\n",
    "    #length of the flattened dimensions\n",
    "    flattened_shape=np.array(tensor_shape[1:]).prod()\n",
    "    flattenoutput=tf.reshape(x_tensor,[tf.shape(x_tensor)[0],flattened_shape])\n",
    "    return flattenoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "\n",
    "    flattened_shape=np.array(x_tensor.get_shape().as_list()[1:]).prod()\n",
    "    weights=tf.Variable(tf.truncated_normal([flattened_shape,num_outputs],stddev=0.04))\n",
    "    bias=tf.Variable(tf.zeros([num_outputs]))\n",
    "    \n",
    "    fc=tf.nn.relu(tf.add(tf.matmul(x_tensor,weights),bias))\n",
    "    return fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "\n",
    "    flattened_shape=np.array(x_tensor.get_shape().as_list()[1:]).prod()\n",
    "    weights=tf.Variable(tf.truncated_normal([flattened_shape,num_outputs],stddev=0.04))\n",
    "    bias=tf.Variable(tf.zeros([num_outputs]))  \n",
    "\n",
    "    outputlayer=tf.add(tf.matmul(x_tensor,weights),bias)    \n",
    "    return outputlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    conv = conv2d_maxpool(x,conv_num_outputs=128,conv_ksize=[5,5],conv_strides=[1,1],pool_ksize=[3,3],pool_strides=[2,2])\n",
    "    conv = conv2d_maxpool(conv,conv_num_outputs=64,conv_ksize=[5,5],conv_strides=[1,1],pool_ksize=[3,3],pool_strides=[2,2])\n",
    "    conv = conv2d_maxpool(conv,conv_num_outputs=64,conv_ksize=[3,3],conv_strides=[1,1],pool_ksize=[3,3],pool_strides=[2,2])\n",
    "\n",
    "    #   Flatten layer\n",
    "    flattened_conv=flatten(conv)\n",
    "    \n",
    "\n",
    "    # Fully Connected Layer\n",
    "    fc=fully_conn(flattened_conv,384)\n",
    "    \n",
    "    # Dropout\n",
    "    fc=tf.nn.dropout(fc,keep_prob)    \n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    fc=fully_conn(fc,192)\n",
    "\n",
    "    return output(fc,10)\n",
    "\n",
    "\n",
    "## Building the Network\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# logits Tensor, so that this can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss function and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network\n",
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "\n",
    "    session.run(optimizer,feed_dict={x:feature_batch,y:label_batch,keep_prob: keep_probability})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Loss and Validation accuracy.  \n",
    "Not using Dropout regularization during validation and test data. Hence keep_prob is kept 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \n",
    "    loss=sess.run(cost,feed_dict={x: feature_batch, y: label_batch, keep_prob: 1})\n",
    "    valid_acc=sess.run(accuracy,feed_dict={x: valid_features, y: valid_labels, keep_prob: 1})\n",
    "\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss,valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch_size = 256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.1211 Validation Accuracy: 0.279400\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     1.7335 Validation Accuracy: 0.329800\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.4883 Validation Accuracy: 0.389600\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.5138 Validation Accuracy: 0.397600\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.4797 Validation Accuracy: 0.468200\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.5629 Validation Accuracy: 0.489000\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.3243 Validation Accuracy: 0.436400\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.0170 Validation Accuracy: 0.525800\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.1428 Validation Accuracy: 0.519600\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.2342 Validation Accuracy: 0.541000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.2275 Validation Accuracy: 0.548800\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.0156 Validation Accuracy: 0.566000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     0.8210 Validation Accuracy: 0.541200\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     0.8932 Validation Accuracy: 0.588400\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.0225 Validation Accuracy: 0.582000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     0.9577 Validation Accuracy: 0.591800\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     0.7481 Validation Accuracy: 0.618400\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     0.5976 Validation Accuracy: 0.594600\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     0.7478 Validation Accuracy: 0.623200\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     0.7783 Validation Accuracy: 0.628400\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     0.7015 Validation Accuracy: 0.660400\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     0.5604 Validation Accuracy: 0.650800\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     0.4005 Validation Accuracy: 0.661200\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     0.6032 Validation Accuracy: 0.648400\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     0.5923 Validation Accuracy: 0.678400\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     0.5297 Validation Accuracy: 0.656000\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     0.5087 Validation Accuracy: 0.663400\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     0.3345 Validation Accuracy: 0.659400\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     0.5041 Validation Accuracy: 0.669000\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     0.4464 Validation Accuracy: 0.672600\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     0.4326 Validation Accuracy: 0.694000\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     0.3627 Validation Accuracy: 0.681400\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     0.2564 Validation Accuracy: 0.675000\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     0.3844 Validation Accuracy: 0.702800\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     0.3579 Validation Accuracy: 0.689600\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     0.4661 Validation Accuracy: 0.688800\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     0.3443 Validation Accuracy: 0.688600\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     0.1995 Validation Accuracy: 0.696400\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     0.3237 Validation Accuracy: 0.704800\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     0.2674 Validation Accuracy: 0.709200\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.3352 Validation Accuracy: 0.715000\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     0.3136 Validation Accuracy: 0.693600\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.2692 Validation Accuracy: 0.711000\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     0.3690 Validation Accuracy: 0.679800\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     0.1968 Validation Accuracy: 0.712400\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.2363 Validation Accuracy: 0.722400\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     0.2415 Validation Accuracy: 0.714200\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.1666 Validation Accuracy: 0.721800\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.2309 Validation Accuracy: 0.707400\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     0.1573 Validation Accuracy: 0.725000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     0.2529 Validation Accuracy: 0.724000\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     0.1973 Validation Accuracy: 0.709200\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     0.1294 Validation Accuracy: 0.731800\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     0.1924 Validation Accuracy: 0.732000\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     0.1291 Validation Accuracy: 0.731200\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     0.1927 Validation Accuracy: 0.702400\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     0.1634 Validation Accuracy: 0.742400\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     0.1169 Validation Accuracy: 0.722400\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     0.1722 Validation Accuracy: 0.731600\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     0.0949 Validation Accuracy: 0.734600\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     0.1540 Validation Accuracy: 0.719000\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     0.1241 Validation Accuracy: 0.739800\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     0.1131 Validation Accuracy: 0.728600\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     0.1703 Validation Accuracy: 0.730400\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     0.0950 Validation Accuracy: 0.720200\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     0.1657 Validation Accuracy: 0.724400\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     0.1223 Validation Accuracy: 0.743000\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     0.0955 Validation Accuracy: 0.726600\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     0.1822 Validation Accuracy: 0.723400\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     0.0621 Validation Accuracy: 0.742200\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     0.1324 Validation Accuracy: 0.746800\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     0.0946 Validation Accuracy: 0.737400\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     0.0910 Validation Accuracy: 0.725800\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     0.1017 Validation Accuracy: 0.734200\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     0.0784 Validation Accuracy: 0.734800\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     0.1414 Validation Accuracy: 0.731400\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     0.0759 Validation Accuracy: 0.741600\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     0.1038 Validation Accuracy: 0.730600\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     0.1140 Validation Accuracy: 0.709600\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     0.0591 Validation Accuracy: 0.735600\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.1494 Validation Accuracy: 0.731000\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     0.0885 Validation Accuracy: 0.734400\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     0.0821 Validation Accuracy: 0.736800\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     0.0805 Validation Accuracy: 0.743800\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     0.0684 Validation Accuracy: 0.724400\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.1028 Validation Accuracy: 0.736000\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     0.0580 Validation Accuracy: 0.746200\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     0.0650 Validation Accuracy: 0.738000\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     0.0632 Validation Accuracy: 0.743000\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     0.0480 Validation Accuracy: 0.735600\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.1081 Validation Accuracy: 0.735400\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     0.0573 Validation Accuracy: 0.737400\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     0.0581 Validation Accuracy: 0.719600\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     0.0555 Validation Accuracy: 0.727400\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     0.0561 Validation Accuracy: 0.727000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.0749 Validation Accuracy: 0.740400\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     0.0955 Validation Accuracy: 0.731000\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.0620 Validation Accuracy: 0.738800\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     0.0770 Validation Accuracy: 0.725000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     0.0645 Validation Accuracy: 0.726600\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.0860 Validation Accuracy: 0.749000\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     0.0579 Validation Accuracy: 0.734400\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.0868 Validation Accuracy: 0.731600\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     0.0727 Validation Accuracy: 0.743400\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     0.0348 Validation Accuracy: 0.738000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.0956 Validation Accuracy: 0.737800\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     0.0874 Validation Accuracy: 0.697400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.0485 Validation Accuracy: 0.727200\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     0.0538 Validation Accuracy: 0.743600\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     0.0330 Validation Accuracy: 0.734400\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.0898 Validation Accuracy: 0.738200\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     0.0591 Validation Accuracy: 0.718600\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.0428 Validation Accuracy: 0.727400\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     0.0785 Validation Accuracy: 0.728400\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     0.0380 Validation Accuracy: 0.741400\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.1262 Validation Accuracy: 0.735200\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     0.0363 Validation Accuracy: 0.750200\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.0423 Validation Accuracy: 0.742000\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     0.0506 Validation Accuracy: 0.741400\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     0.0515 Validation Accuracy: 0.740400\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.0848 Validation Accuracy: 0.723400\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     0.0477 Validation Accuracy: 0.735600\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.0465 Validation Accuracy: 0.740400\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.0451 Validation Accuracy: 0.735800\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     0.0459 Validation Accuracy: 0.742400\n"
     ]
    }
   ],
   "source": [
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Saving Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
